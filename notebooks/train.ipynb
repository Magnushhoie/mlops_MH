{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import os, glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "verbose = True\n",
    "subset = True\n",
    "subset_fraction = 0.10\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\") # Temp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import src.models.tools"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataDir = \"../data/processed/\"\n",
    "\n",
    "# Params\n",
    "batch_size = 64\n",
    "\n",
    "# Load datasets\n",
    "trainset = torch.load(dataDir + \"train.pt\")\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size, shuffle=True)\n",
    "\n",
    "validset = torch.load(dataDir + \"valid.pt\")\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size, shuffle=True)\n",
    "\n",
    "testset = torch.load(dataDir + \"test.pt\")\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "images, labels = next(iter(trainloader))\n",
    "image, label = np.squeeze(images[0], 0), labels[0]\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('%i' % label)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOZklEQVR4nO3da6hd9ZnH8d/PGBmTlEycaIypTWqVcVScRIIXqsWx2GQEUcEWQ15knIH0RSNT8MVIB62iMsOglXEGCqkGU62xBeMojTRGmdGOkeJRHE201tRbooeEmOCFMOb2zIuzMhzjWf99sm9r5zzfD2z25Tlrr4dNfllr7f9e6++IEICJ75imGwDQH4QdSIKwA0kQdiAJwg4kQdiBJAg7kARhx5hs/5ft/7X9WXV7s+me0BnCjpIVETGtuv15082gM4QdSIKwo+SfbO+0/bztS5tuBp0xv43HWGxfIOl1SXslXSfp3yXNj4g/NtoY2kbYMS62fyNpXUT8W9O9oD3sxmO8QpKbbgLtI+z4Ett/anuR7T+xfaztpZK+JWl9072hfcc23QAG0mRJd0g6U9IBSb+XdHVEMNZ+FOOYHUiC3XggCcIOJEHYgSQIO5BEX7+Nt823gUCPRcSYv4foaMtue7HtN21vsX1TJ+8FoLfaHnqzPUnSHyRdLmmbpBclLYmI1wvLsGUHeqwXW/bzJW2JiLcjYq+kRyRd1cH7AeihTsI+R9LWUc+3Va99ge3ltodsD3WwLgAd6uQLurF2Fb60mx4RKyWtlNiNB5rUyZZ9m6RTRz3/qqQPO2sHQK90EvYXJZ1h++u2j9PIBQ6e6E5bALqt7d34iNhve4VGTnucJGlVRGzuWmcAuqqvZ71xzA70Xk9+VAPg6EHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJ9nbIZGO3EE08s1qdPn96zdb/33nvF+r59+3q27qawZQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJJjFFY3ZsmVLsX7aaaf1bN3r168v1lv1dsMNN3Szna6qm8W1ox/V2H5X0qeSDkjaHxELO3k/AL3TjV/Q/VVE7OzC+wDoIY7ZgSQ6DXtIesr2S7aXj/UHtpfbHrI91OG6AHSg0934b0bEh7ZPkrTB9u8j4rnRfxARKyWtlPiCDmhSR1v2iPiwut8h6TFJ53ejKQDd13bYbU+1/ZVDjyV9R9KmbjUGoLs62Y2fJekx24fe5+GI+E1XusJR4+STTy7Wly5dWlvr5Th6K4sWLSrWh4eH+9RJ/7Qd9oh4W9JfdrEXAD3E0BuQBGEHkiDsQBKEHUiCsANJcIrrBDdjxoyOlt+9e3exfu+99xbrK1asaHvdzz//fLF+9913F+vvvPNO2+vevHlzsb5///6237vX6k5xZcsOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwZfMEUJra+OGHHy4u+/TTTxfrrcayzznnnGK9ZO7cucX6Rx99VKzv2bOn7XVnxJYdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnH0CuOOOO2prrS6Z/OSTT3a07ttuu61YX7ZsWW1t69atHa0bR4YtO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXXjJ4B9+/bV1o45pvz/+WWXXVasP/vss8X6cccdV6zv3bu3WEf3tX3deNurbO+wvWnUayfY3mD7req+s5kIAPTceHbjH5C0+LDXbpL0TEScIemZ6jmAAdYy7BHxnKRdh718laTV1ePVkq7ublsAuq3d38bPiohhSYqIYdsn1f2h7eWSlre5HgBd0vMTYSJipaSVEl/QAU1qd+htu+3ZklTd7+heSwB6od2wPyHp0LmLyyQ93p12APRKy3F222skXSpppqTtkn4s6T8k/UrS1yS9L+m7EXH4l3hjvRe78W2YNGlSsV4aZ2/lqaeeKtYXLz58IAaDrm6cveUxe0QsqSl9u6OOAPQVP5cFkiDsQBKEHUiCsANJEHYgCS4lPQCmTp1arN9yyy09W/f27duL9bPOOqtY//jjj4v1Dz74oLbW6vTbgwcPFus4MmzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJLiU9AJYsqTuxcMRDDz1UrG/cuLG2NmXKlOKy5513XrHe6t/Htm3bivX77ruvttbqMtZ33nlnsb5hw4ZiPau2LyUNYGIg7EAShB1IgrADSRB2IAnCDiRB2IEkGGc/Ctx+++3F+s0331xbazXOvmDBgmJ96dKlxfqcOXOK9UWLFtXWJk+eXFz2wIEDxfo111xTrK9bt65Yn6gYZweSI+xAEoQdSIKwA0kQdiAJwg4kQdiBJBhnR0+VztVfvXp1cdljjy1Pa3DPPfcU6zfeeGOxPlG1Pc5ue5XtHbY3jXrtVtsf2H6lul3RzWYBdN94duMfkLR4jNfviYj51e3J7rYFoNtahj0inpO0qw+9AOihTr6gW2H71Wo3f0bdH9lebnvI9lAH6wLQoXbD/lNJ35A0X9KwpLvr/jAiVkbEwohY2Oa6AHRBW2GPiO0RcSAiDkr6maTzu9sWgG5rK+y2Z496eo2kTXV/C2AwtJyf3fYaSZdKmml7m6QfS7rU9nxJIeldSd/vXYs4mq1Zs6a29uCDD3b03q3Oh8cXtQx7RIz1q4j7e9ALgB7i57JAEoQdSIKwA0kQdiAJwg4kwSmulRdeeKFYv+iii/rUSR7r168v1i+//PJifffu3cX6BRdcUFvbsmVLcdmjGZeSBpIj7EAShB1IgrADSRB2IAnCDiRB2IEkWp71lkWr3xuUxmXPPffc4rJ79uxpq6eJ4JRTTml72c8//7xYv/7664v1iTyW3g627EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOezVz755JNifdq0abW1HTt2FJe96667ivUHHnigWN+5c2exvnBh/WQ7+/fvLy7ba2vXrq2tzZs3r7jsunXrivUrr7yynZYmPM5nB5Ij7EAShB1IgrADSRB2IAnCDiRB2IEkWo6z2z5V0s8lnSzpoKSVEfGvtk+Q9EtJ8zQybfP3IqJ4Ie9BHme/8MILi/VVq1bV1s4888yO1v3+++8X6xs3bizWr7vuuo7W35StW7cW63Pnzu1TJxNLJ+Ps+yXdGBF/IelCST+wfZakmyQ9ExFnSHqmeg5gQLUMe0QMR8TL1eNPJb0haY6kqyStrv5staSre9QjgC44omN22/MkLZD0O0mzImJYGvkPQdJJXe8OQNeM+xp0tqdJelTSDyPiE3vMw4KxllsuaXl77QHolnFt2W1P1kjQfxERh85s2G57dlWfLWnMs0EiYmVELIyI+rM1APRcy7B7ZBN+v6Q3IuIno0pPSFpWPV4m6fHutwegW8Yz9HaxpN9Kek0jQ2+S9CONHLf/StLXJL0v6bsRsavFew3s0FsrU6ZMqa1dcsklxWVbDY1de+21xfrUqVOL9UceeaS2Vjr9VZJOP/30Yr2V4eHhYr00bNjqFNZWp/5ibHVDby2P2SPivyXVHaB/u5OmAPQPv6ADkiDsQBKEHUiCsANJEHYgCcIOJMGlpAfA2WefXawff/zxxfrQ0FBtbebMmcVlp0+fXqy30mo66lbj8Og+LiUNJEfYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzg5MMIyzA8kRdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBItw277VNv/afsN25tt/331+q22P7D9SnW7ovftAmhXy4tX2J4taXZEvGz7K5JeknS1pO9J+iwi7hr3yrh4BdBzdRevOHYcCw5LGq4ef2r7DUlzutsegF47omN22/MkLZD0u+qlFbZftb3K9oyaZZbbHrJdP0cRgJ4b9zXobE+T9KykOyNire1ZknZKCkm3a2RX/29bvAe78UCP1e3GjyvstidL+rWk9RHxkzHq8yT9OiLOafE+hB3osbYvOGnbku6X9MbooFdf3B1yjaRNnTYJoHfG8238xZJ+K+k1SQerl38kaYmk+RrZjX9X0verL/NK78WWHeixjnbju4WwA73HdeOB5Ag7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJtLzgZJftlPTeqOczq9cG0aD2Nqh9SfTWrm72Nreu0Nfz2b+0cnsoIhY21kDBoPY2qH1J9NaufvXGbjyQBGEHkmg67CsbXn/JoPY2qH1J9NauvvTW6DE7gP5pessOoE8IO5BEI2G3vdj2m7a32L6piR7q2H7X9mvVNNSNzk9XzaG3w/amUa+dYHuD7beq+zHn2Guot4GYxrswzXijn13T05/3/Zjd9iRJf5B0uaRtkl6UtCQiXu9rIzVsvytpYUQ0/gMM29+S9Jmknx+aWsv2v0jaFRH/XP1HOSMi/mFAertVRziNd496q5tm/G/U4GfXzenP29HElv18SVsi4u2I2CvpEUlXNdDHwIuI5yTtOuzlqyStrh6v1sg/lr6r6W0gRMRwRLxcPf5U0qFpxhv97Ap99UUTYZ8jaeuo59s0WPO9h6SnbL9ke3nTzYxh1qFptqr7kxru53Atp/Hup8OmGR+Yz66d6c871UTYx5qaZpDG/74ZEedJ+mtJP6h2VzE+P5X0DY3MATgs6e4mm6mmGX9U0g8j4pMmexltjL768rk1EfZtkk4d9fyrkj5soI8xRcSH1f0OSY9p5LBjkGw/NINudb+j4X7+X0Rsj4gDEXFQ0s/U4GdXTTP+qKRfRMTa6uXGP7ux+urX59ZE2F+UdIbtr9s+TtJ1kp5ooI8vsT21+uJEtqdK+o4GbyrqJyQtqx4vk/R4g718waBM4103zbga/uwan/48Ivp+k3SFRr6R/6Okf2yih5q+TpP0P9Vtc9O9SVqjkd26fRrZI/o7SX8m6RlJb1X3JwxQbw9qZGrvVzUSrNkN9XaxRg4NX5X0SnW7ounPrtBXXz43fi4LJMEv6IAkCDuQBGEHkiDsQBKEHUiCsANJEHYgif8DWpWm/8lvDdQAAAAASUVORK5CYII="
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test out your network!\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "# TODO: Calculate the class probabilities (softmax) for img\n",
    "ps = torch.tensor([0.10]*10)\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADsCAYAAAAhDDIOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUkklEQVR4nO3de5RldXnm8e9D06DNpWG4GG7SoEhQRwJ2EFQYsZEAMaKJMSCSkbhkNOCAGhPiwkBiVpZOsojOiJoexEsikKB4jSgYBsGRi93coUGRm02jgCjQtALdvPPHOWTVVGo31cU+tfdpvp+1alFnv/uceqrWad56f2fX+aWqkCSpbzboOoAkSVOxQUmSeskGJUnqJRuUJKmXbFCSpF6yQUmSeskGJWlkkpya5J+6zrGukixIUkk2nOH9K8nzG2pHJblgqnOTfDLJB2aWev1jg5L0tCR5c5IlSVYmuSfJ+Ule2VGWSvLIMMvdSU5LMqeLLE2q6vNVdXBD7R1V9UGAJK9Ksnx20/WLDUrSjCV5D/AR4G+A5wDPBT4OHN5hrD2ralNgEfBm4O2TT5jpZKTZZYOSNCNJ5gN/BRxXVedV1SNV9XhVfa2q3tdwn3OT/CTJg0kuSfKiCbXDktyU5OHh9PMnw+NbJ/l6kl8keSDJpUme8v9dVXUzcCnw4glLdm9LchdwUZINkpyc5M4k9yb53PB7muiPkqwYTobvnZB1nySXDTPdk+RjSTaadN/DktyW5P4kf/tk5iRvTfLdhp/PZ5L8dZJNgPOB7YfT4Mok2ydZlWSrCee/NMl9SeY+1c9jHNmgJM3UfsCzgC+tw33OB3YDtgWuAj4/ofYp4L9V1WbAi4GLhsffCywHtmEwpb0feMr3aEvyQmB/4OoJh/8LsAfwW8Bbhx8HArsCmwIfm/QwBw7zHgyclOSg4fE1wLuBrRn8HBYBfzzpvm8AFgJ7M5go/+ipMj+pqh4BDgVWVNWmw48VwMXAmyac+hbgnKp6fLqPPU5sUJJmaivg/qpaPd07VNWZVfVwVT0KnArsOWFqeRx4YZLNq+rnVXXVhOPbATsPJ7RLa+1vInpVkp8DXwPOAD49oXbqcNL7JXAUcFpV3VZVK4E/B46YtPz3l8Pzrx8+zpHD72NpVV1eVaur6g7gHxg0v4k+XFUPVNVdDJZBj5zuz2ktPsugKTF8be1I4B9beNxeskFJmqmfAVtP9/WcJHOSfCjJj5I8BNwxLG09/O/vAYcBdyb5TpL9hsf/FrgVuGC4ZHbSU3ypvatqy6p6XlWdXFVPTKj9eMLn2wN3Trh9J7AhgyltqvPvHN6HJC8YLjv+ZPi9/M2E72Ot932avsKgie8KvAZ4sKqubOFxe8kGJWmmLgN+Bbx+mue/mcFS10HAfGDB8HgAqur7VXU4g+W/LwP/Mjz+cFW9t6p2BX4HeE+SRTPMPHHyWgHsPOH2c4HVwE8nHNtpUn3F8PNPADcDu1XV5gyWHTPpazXddyZZBweqfsXg53IUcDTr8fQENihJM1RVDwJ/AZye5PVJ5iWZm+TQJP9jirtsBjzKYPKax2DqACDJRsO/D5o/fD3lIQav85DktUmenyQTjq9p4Vs4G3h3kl2SbDrM88+Tliw/MPy+XgQcA/zzhO/lIWBlkl8H3jnF478vyZZJdgJOmHDf6fopsNUUF258jsFrZ68Dxu5vzNaFDUrSjFXVacB7gJOB+xgsax3PYAKa7HMMlrruBm4CLp9UPxq4Y7hk9g6Gr7UwuEjh28BKBlPbx6vq4hbin8lgArkEuJ3BNPiuSed8h8Hy4r8Bf1dVT/6B7Z8wmAgfBv43UzefrwBLgWuAf2VwEci0Da9CPBu4bXi14PbD4/8XeAK4avj613orblgoSeMlyUXAWVV1RtdZRskGJUljJMlvAhcCO1XVw13nGSWX+CRpTCT5LIPlzhPX9+YETlCSpJ5a698vvGaD37d76RnvwifOnXz5sKRZ4BKfJKmXfEdfqUNbb711LViwoOsYUqeWLl16f1VtM/m4DUrq0IIFC1iyZEnXMaROJblzquMu8UmSeskGJUnqJRuUJKmXbFCSpF6yQUmSeskGJUnqJS8zlzp0/d0PsuCkf33aj3PHh367hTRSvzhBSZJ6yQYlSeolG5QkqZdsUFLLkpyQ5IYkNyY5ses80riyQUktSvJi4O3APsCewGuT7NZtKmk82aCkdu0BXF5Vq6pqNfAd4A0dZ5LGkg1KatcNwAFJtkoyDzgM2GniCUmOTbIkyZI1qx7sJKQ0Dvw7KKlFVbUsyYeBC4GVwLXA6knnLAYWA2y83W7uWi01cIKSWlZVn6qqvavqAOAB4IddZ5LGkROU1LIk21bVvUmeC/wusF/XmaRxZIOS2vfFJFsBjwPHVdXPuw4kjSMblNSyqtq/6wzS+sDXoCRJveQEJXXoP+8wnyW+E7k0JScoSVIv2aAkSb1kg5Ik9ZKvQUkdckddqZkTlCSpl2xQkqReskFJLUvy7uFmhTckOTvJs7rOJI0jG5TUoiQ7AP8dWFhVLwbmAEd0m0oaTzYoqX0bAs9OsiEwD1jRcR5pLHkVX4s2/LXnNNZuOmXnxtrzz3q8sbbBpVc/rUyaXVV1d5K/A+4CfglcUFUXdBxLGktOUFKLkmwJHA7sAmwPbJLkLZPOcUddaRpsUFK7DgJur6r7qupx4Dzg5RNPqKrFVbWwqhbOmTe/k5DSOLBBSe26C9g3ybwkARYByzrOJI0lG5TUoqq6AvgCcBVwPYN/Y4s7DSWNKS+SkFpWVacAp3SdQxp3TlCSpF5yglpHt5/zksbajft/urG2718d31jb4NIrn1YmSVof2aCkDrmjrtTMJT5JUi/ZoCRJveQSn9QhNyyUmjlBSZJ6yQlqHc3/1ibNxf1nL0cX7nvnfut8n6s+8InG2mGLfr+xtmbZD9f5a0lavzhBSZJ6yQYltSjJ7kmumfDxUJITu84ljSOX+KQWVdUtwG8AJJkD3A18qctM0rhygpJGZxHwo6q6s+sg0jiyQUmjcwRw9uSDblgoTY8NShqBJBsBrwPOnVxzw0JpenwNSq244uSPTXn8sEVHNN5nPb+U/FDgqqr6addBpHHlBCWNxpFMsbwnafpsUFLLkswDXgOc13UWaZy5xCe1rKpWAVt1nUMad05QkqRecoKSOuSGhVIzJyhJUi85QU1hzot2b6x976+nvpwaYJ8lRzXWtv2Hy55Wpj5YdeDKriNIegZxgpIk9ZITlNQhd9SVmjlBSZJ6yQYlSeolG5TUsiRbJPlCkpuTLEuyX9eZpHHka1BS+z4KfLOq3jh8V/N5XQeSxpENagr55aMzut9Ze57ZWDvuoHc11uZ+e+mMvt5MrfrdlzXWTj/to421PebObs5xlGRz4ADgrQBV9RjwWJeZpHHlEp/Url2B+4BPJ7k6yRlJNuk6lDSObFBSuzYE9gY+UVV7AY8AJ008wR11pemxQUntWg4sr6orhre/wKBh/Tt31JWmxwYltaiqfgL8OMmT75e1CLipw0jS2PIiCal97wI+P7yC7zbgmI7zSGPJBiW1rKquARZ2nUMadzaoKay+7Y7G2nWPrWmsvWSjjRtrPzt+VWPt1749rVj/wZwtt2wu1hONpd/74Lcaa3vMnTujLIcfdMSUx9cs++GMHk+SfA1KktRLTlBSh9xRV2rmBCVJ6iUblCSpl1zikzrkhoVSMycoSVIvOUFNYc7mmzfWXrLRnMbarY83vwv6dps/1PwF93rRtHJNds+pzZe8J9VYO26Li2b09W5f/avm4uOrZ/SYktTECUqS1EtOUFLLktwBPAysAVZXle8qIc2ADUoajQOr6v6uQ0jjzCU+SVIv2aCk9hVwQZKlSY6dXHTDQml6XOKT2veKqlqRZFvgwiQ3V9UlTxarajGwGGDj7XZrvtxSeoazQU1hzUPNl4Qfc+eixtqnd/63xtrXdv9qY+3q85rfeXyvjZqH3At+uUlj7eBnP9JYW5sXfPWdzbV3XrmWe94+o6+3PqqqFcP/3pvkS8A+wCVrv5ekyVzik1qUZJMkmz35OXAwcEO3qaTx5AQltes5wJeSwODf11lV9c1uI0njyQYltaiqbgP27DqHtD5wiU+S1EtOUFKH3LBQauYEJUnqJSeodXTfiTs11vb80//aWLt2v8821tZ2Kfkp9+7VWHvFZj9orEnSuHOCkiT1khOU1CF31JWaOUFJknrJBiVJ6iUblCSpl2xQ0ggkmZPk6iRf7zqLNK68SGIdzfnBXY21Bcdt3Fj7g3MPaaytWPy8GWU5f/4rG2sHv/9/NdaufDSNtd0XN78LuvtCrJMTgGXA5l0HkcaVE5TUsiQ7Ar8NnNF1Fmmc2aCk9n0E+FNgyo2+3FFXmh4blNSiJK8F7q2qpU3nVNXiqlpYVQvnzJs/i+mk8WKDktr1CuB1Se4AzgFeneSfuo0kjScblNSiqvrzqtqxqhYARwAXVdVbOo4ljSUblCSpl7zMfB2t+cXMXtR+5IDm2nzua6zdderLG2vXvb35UvKXLX1zY22b193SHIYb11LTuqiqi4GLO44hjS0nKElSLzlBSR1yR12pmROUJKmXbFCSpF5yiU/qkBsWSs2coCRJveQE1QMb7rxTY+3df/Dl2QsiST3iBCVJ6iUblNSiJM9KcmWSa5PcmOQvu84kjSuX+KR2PQq8uqpWJpkLfDfJ+VV1edfBpHFjg5JaVFUFrBzenDv8cDNiaQZc4pNalmROkmuAe4ELq+qKjiNJY8kGJbWsqtZU1W8AOwL7JHnxxLo76krT4xLfLFn5pn0baxf//emzmESzpap+keRi4BDghgnHFwOLATbebjeX/6QGTlBSi5Jsk2SL4efPBg4Cbu40lDSmnKCkdm0HfDbJHAa/AP5LVX2940zSWLJBSS2qquuAvbrOIa0PXOKTJPWSDUqS1Esu8UkdckddqZkNaoztcdbxjbXnve+yWUwiSe1ziU+S1EtOUFKH3FFXauYEJUnqJRuUJKmXbFCSpF6yQUktSrJTkv+TZNlwR90Tus4kjSsvkhhju563qusI+o9WA++tqquSbAYsTXJhVd3UdTBp3DhBSS2qqnuq6qrh5w8Dy4Aduk0ljScblDQiSRYweOPYKyYdd8NCaRpsUNIIJNkU+CJwYlU9NLFWVYuramFVLZwzb343AaUxYIOSWpZkLoPm9PmqOq/rPNK4skFJLUoS4FPAsqo6res80jjzKr4WrXzTvo21i//+9FlMog69AjgauD7JNcNj76+qb3QXSRpPNiipRVX1XSBd55DWBy7xSZJ6yQlK6pAbFkrNnKAkSb1kg5Ik9ZINSpLUS74G1aJtjrt9Rvc77ObXN9Zy8n9qrl127Yy+nvrDHXWlZk5QkqReskFJknrJBiW1KMmZSe5NckPXWaRxZ4OS2vUZ4JCuQ0jrAxuU1KKqugR4oOsc0vrABiVJ6iUvM2/RdTcsaC4+v7m06vG5jbXNvn9jY62mkUn9k+RY4FiAOZtv03Eaqb+coKRZ5o660vTYoCRJvWSDklqU5GzgMmD3JMuTvK3rTNK48jUoqUVVdWTXGaT1hROUJKmXbFCSpF5yia9Ft7z+4421Q44+trH2rLU8Zq2+7WkkUt+5o67UzAlKktRLNihJUi+5xCd1yA0LpWZOUJKkXrJBSZJ6yQYlSeolX4Nq0Wt3eGljbUOWzmISdSnJIcBHgTnAGVX1oY4jSWPJCUpqUZI5wOnAocALgSOTvLDbVNJ4skFJ7doHuLWqbquqx4BzgMM7ziSNJRuU1K4dgB9PuL18eOzfJTk2yZIkS9asenBWw0njxAYltStTHPv/Nj92w0JpemxQUruWAztNuL0jsKKjLNJYs0FJ7fo+sFuSXZJsBBwBfLXjTNJY8jJzqUVVtTrJ8cC3GFxmfmZV3dhxLGks2aCkllXVN4BvdJ1DGncu8UmSeskJSuqQGxZKzZygJEm9ZIOSJPWSDUqS1Es2KElSL9mgJEm9ZIOSJPWSDUqS1Es2KElSL/mHulKHli5dujLJLV3nmGBr4P6uQwyZZWrrY5adpzpog5K6dUtVLew6xJOSLOlLHrNM7ZmUZa0N6sInzp1q8zVJkkbO16AkSb1kg5K6tbjrAJP0KY9ZpvaMyZKqGuXjS5I0I05QkqReskFJsyDJIUluSXJrkpOmqCfJ/xzWr0uyd4dZjhpmuC7J95Ls2VWWCef9ZpI1Sd7YZZYkr0pyTZIbk3xnVFmmkyfJ/CRfS3LtMM8xI8pxZpJ7k9zQUB/dc7eq/PDDjxF+AHOAHwG7AhsB1wIvnHTOYcD5QIB9gSs6zPJyYMvh54d2mWXCeRcB3wDe2OHPZQvgJuC5w9vbdvyceT/w4eHn2wAPABuNIMsBwN7ADQ31kT13naCk0dsHuLWqbquqx4BzgMMnnXM48LkauBzYIsl2XWSpqu9V1c+HNy8HdhxBjmllGXoX8EXg3hHlmG6WNwPnVdVdAFXVdZ4CNksSYFMGDWp120Gq6pLhYzcZ2XPXBiWN3g7AjyfcXj48tq7nzFaWid7G4LfjUXjKLEl2AN4AfHJEGaadBXgBsGWSi5MsTfKHHef5GLAHsAK4Hjihqp4YYaYmI3vu+k4S0uhN9Qfvky+fnc45s5VlcGJyIIMG9coR5Jhulo8Af1ZVawaDwshMJ8uGwEuBRcCzgcuSXF5VP+goz28B1wCvBp4HXJjk0qp6aAR51mZkz10blDR6y4GdJtzekcFvvet6zmxlIclLgDOAQ6vqZyPIMd0sC4Fzhs1pa+CwJKur6ssdZFkO3F9VjwCPJLkE2BMYRYOaTp5jgA/V4IWgW5PcDvw6cOUI8qzNyJ67LvFJo/d9YLckuyTZCDgC+Oqkc74K/OHwiqh9gQer6p4usiR5LnAecPSIpoNpZ6mqXapqQVUtAL4A/PEImtO0sgBfAfZPsmGSecDLgGUjyDLdPHcxmOZI8hxgd+C2EeVZm5E9d52gpBGrqtVJjge+xeDqrDOr6sYk7xjWP8ngCrXDgFuBVQx+O+4qy18AWwEfH04uq2sEbwg6zSyzYjpZqmpZkm8C1wFPAGdU1ZSXXs9GHuCDwGeSXM9gme3Pqqr1dzlPcjbwKmDrJMuBU4C5E3KM7LnrO0lIknrJJT5JUi/ZoCRJvWSDkiT1kg1KktRLNihJUi/ZoCRJvWSDkiT1kg1KktRL/w9G3KBAtPMtBgAAAABJRU5ErkJggg=="
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(32,64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(3*3*64, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x),2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.view(-1,3*3*64 )\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    " \n",
    "cnn = CNN()\n",
    "print(cnn)\n",
    "\n",
    "it = iter(trainloader)\n",
    "X_batch, y_batch = next(it)\n",
    "print(cnn.forward(X_batch).shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def fit(model):\n",
    "    global epochs, criterion, optimizer\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1} / {epochs}\")\n",
    "\n",
    "        correct = 0\n",
    "        running_loss = 0\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            ps = torch.exp(model(images))\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "        mean_loss = running_loss / len(trainset)\n",
    "        mean_accuracy = (100 * correct / len(trainset)).item()\n",
    "        print(f\"Training Loss: {mean_loss:.6f}, Accuracy: {mean_accuracy:.2f}\")\n",
    "\n",
    "        # Stats\n",
    "        loss_list.append(mean_loss)\n",
    "        accuracy_list.append(mean_accuracy)\n",
    "\n",
    "    return loss_list, accuracy_list\n",
    "\n",
    "        # Evaluate\n",
    "            # Total correct predictions\n",
    "            #predicted = torch.max(output.data, 1)[1] \n",
    "            #correct += (predicted == labels).sum()\n",
    "\n",
    "            #print(correct)\n",
    "            #if i % 50 == 0:\n",
    "            #    print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "            #        epoch, i*len(X_batch), len(trainloader.dataset), 100.*i / len(trainloader), loss.data[0], float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "                "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())#,lr=0.001, betas=(0.9,0.999))\n",
    "\n",
    "def test_model(model, trainloader, criterion, optimizer):\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = next(dataiter)\n",
    "\n",
    "    # Forward pass, get our logits\n",
    "    logits = model(images)\n",
    "\n",
    "    # Calculate the loss with the logits and the labels\n",
    "    loss = criterion(logits, labels)\n",
    "    print(loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import src.models"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "src.models.tools.test_model(model, trainloader, criterion, optimizer, v=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = CNN()\n",
    "\n",
    "# Params\n",
    "epochs = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())#,lr=0.001, betas=(0.9,0.999))\n",
    "\n",
    "# Test\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "print(loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(2.2949, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Saving model ...\")\n",
    "print(\"Model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Our model: \n",
      "\n",
      " CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'conv3.weight', 'conv3.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def save_checkpoint(model, filepath=\"models/checkpoint.pth\"):\n",
    "    checkpoint = {\"model\": model,\n",
    "                  \"state_dict\": model.state_dict()}\n",
    "    torch.save(checkpoint, filepath)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = checkpoint[\"model\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "save_checkpoint(model, filepath=\"checkpoint.pth\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model2 = load_checkpoint(filepath=\"checkpoint.pth\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fit(model2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 / 5\n",
      "Training Loss: 0.002662, Accuracy: 94.69\n",
      "Epoch 2 / 5\n",
      "Training Loss: 0.002695, Accuracy: 94.89\n",
      "Epoch 3 / 5\n",
      "Training Loss: 0.002425, Accuracy: 95.40\n",
      "Epoch 4 / 5\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10200/1451791419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_10200/3491165585.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10200/85430886.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#x = F.dropout(x, p=0.5, training=self.training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from src.visualization import visualize\n",
    "\n",
    "visualize.plot_metric(loss_list, \"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"test.pdf\")\n",
    "\n",
    "visualize.plot_metric(accuracy_list, \"Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"test2.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e86a6ad60df81dd235454666848363722e406489cd8c1e998902fcfb7b844f3"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('py39': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}